{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: Text Recommendation or text similarity using Generative AI\n",
    "\n",
    "* Problem statement: Find out the Similar tickets using generative AI\n",
    "\n",
    "* Approach: Using GPT4 and FAISS technique to embed text and find similarity score which helps us finding the most similar text.\n",
    "\n",
    "* Algorithm: Sentence transformers for embedding, FAISS for indexing of the embeddings, gpt4 model for text understanding\n",
    "\n",
    "* Data Structure: Unstructural text data or columnar data\n",
    "\n",
    "* Procedure: \n",
    "    - Select required columns and load a CSV Loader\n",
    "    - Create embeddings for the given text using sentence transformer\n",
    "    - Store or index all the embeddings into Database(local memory) using FAISS\n",
    "    - Test query embedding is tested with existing db to find similarity score\n",
    "    - sort out the similarity score and recommend the text\n",
    "\n",
    "* Libraries installed: GPT4ALL, FAISS-CPU, Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*tmMrwJAPRy1zKG8-k-J_xg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -qq install gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment below lines and install required packages, if faiss-gpu is failed try faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-gpu -qq\n",
    "# !pip install faiss-cpu -qq\n",
    "# !pip install sentence-transformers -qq\n",
    "# !pip install huggingface-hub -qq\n",
    "# !pip install langchain -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT4 model to generate text like chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\",\n",
    "                model_path=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model with sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Paris, the city of light, is located in the northern part of the country and is'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(\"The capital of France is \", max_tokens=20)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model with sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3 billion people, and it has a rapidly growing economy that is expected to reach $5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(\"India is a country with population \", max_tokens=20)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way of loading gpt4 model using langchain, \n",
    "# but not suggested as model_path specification not available\n",
    "from langchain.llms import GPT4All\n",
    "llm = GPT4All(model=\"C:/Users/vjakkula/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\")\n",
    "llm(\"The first man on the moon was ... Let's think step by step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nHere's an example code snippet:\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\\n# Load the data\\ndata = ... # Your data here\\n\\n# Define the target labels\\nlabels = ... # Your labels here\\n\\n# Create an instance of the auto-tokenizer\\ntokenizer = AutoTokenizer.from_pretrained('cardiff')\\n\\n# Convert the data to a numpy array\\nX = tokenizer(data, return_tensors=' batch', input_shape=(None,), output_shapes=[ None, 1 ]).prepare()\\ny = tokenizer(labels, return_tensors='batch', input_shape=(None,)).prepare()\\n\\n# Train the model on the numpy array\\nmodel = AutoModelForSequenceClassification.from_pretrained('cardiff')\\nmodel.fit(X, y)\\n``` \\n\\nNote that you will need to replace `'cardiff'` with the name of your custom tokenizer and `labels` with the names of your target labels.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Use the anomaly detection function from the transformers library to train a model on the numpy array.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# #\\n\\n# Load the data\\n\n",
    "# data = \n",
    "# # Your data here\\n\\n# Define the target labels\\n\n",
    "# labels = ... # Your labels here\\n\\n# Create an instance of the auto-tokenizer\\n\n",
    "# tokenizer = AutoTokenizer.from_pretrained('cardiff')# Convert the data to a numpy array\\n\n",
    "# X = tokenizer(data, return_tensors=' batch', input_shape=(None,), output_shapes=[ None, 1 ]).prepare()\n",
    "# y = tokenizer(labels, return_tensors='batch', input_shape=(None,)).prepare()\n",
    "# #\\n\\n# Train the model on the numpy array\\n\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('cardiff')\n",
    "# model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV Loader or Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('DATA.csv', encoding=\"utf8\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='DATA.csv', encoding=\"utf8\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding of the text using GPT4 model and Huggingface sentence tranformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All, Embed4All\n",
    "embeddings = Embed4All()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, list)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The quick brown fox jumps over the lazy dog'\n",
    "output = embeddings.embed(text)\n",
    "len(output),type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',cache_folder=\"LLM Similar Ticket Detection/SentenceTransformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"LLM Similar Ticket Detection/SentenceTransformers/sentence-transformers_all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelining the embedding and data into database indexing using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will take longer to execute\n",
    "db = FAISS.from_documents(data,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "- Take 1 query from the data and create test embeddings and search with FAISS DB \n",
    "- input : test query\n",
    "- output: similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.sample()[\"Description\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\" Router was unable to resolve \"\"\"\n",
    "result_docs = db.similarity_search_with_score(query,k=10)\n",
    "for idx,doc in enumerate(result_docs):\n",
    "    #if idx == 0:\n",
    "       # continue\n",
    "    print(f\"Recommendation {idx}, score = {round((1-doc[1])*100,2)}%\\n=============================\\n\")\n",
    "    print(doc[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the FAISS index in local memory to use further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the saved FAISS index\n",
    "new_db = FAISS.load_local(\"Recommendation_Streamlit_Web_App/SavedModels/faiss_index_v3\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_docs = new_db.similarity_search_with_score(\"something\")\n",
    "result_docs[0][0].metadata\n",
    "# for doc in result_docs:\n",
    "#     print(doc[0].page_content,\"\\n score = \",doc[1])\n",
    "#     print(doc[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base = os.path.abspath(os.path.dirname(\"LLM Similar Ticket Detection.ipynb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(base,\"Data\",\"53766.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_docs = new_db.similarity_search_with_score(df.iloc[0,:][\"Description\"],k=10)\n",
    "for idx,doc in enumerate(result_docs):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    print(f\"Recommendation {idx}, score = {round((1-doc[1])*100,2)}%\\n=============================\\n\")\n",
    "    print(doc[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 331, 3088, 2649, 10362, 9192, 3937, 3099, 335, 8553]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxes = [r[0].metadata[\"row\"] for r in result_docs]\n",
    "idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkt_data1 = pd.read_excel(\"/Tkt_Recommendation_sep27_Datasets/ISSUE_DATA.xlsx\")\n",
    "tkt_data1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for FAISS text similarity\n",
    "\n",
    "Read the duplicates data and use it to calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_df = pd.read_csv(\"Tkt_Recommendation_sep27_Datasets/duplicates.csv\")\n",
    "dup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_filtered = tkt_data1[tkt_data1[\"Key\"].isin(dup_df[[\"Issue\"]].values.reshape(-1))]\n",
    "ground_filtered[\"Description\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_score(issue,query):\n",
    "    result_docs = new_db.similarity_search_with_score(query,k=5)\n",
    "    idxes = [r[0].metadata[\"row\"] for r in result_docs]\n",
    "    recommended_keys = tkt_data1.iloc[idxes,:][\"Key\"].values\n",
    "    ground_truth_keys = dup_df[dup_df[\"Issue\"]==issue].values.reshape(-1)\n",
    "    return ((sum([1 if each in recommended_keys else 0 for each in ground_truth_keys]))/(len(ground_truth_keys)))*100\n",
    "recommend_score(ground_filtered[\"Key\"].values[0],ground_filtered[\"Description\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_filtered[\"score\"] = [recommend_score(row[0],row[1]) for _,row in ground_filtered[[\"Key\",\"Description\"]].iterrows()]\n",
    "ground_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_filtered[\"score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.26020408163265"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_filtered[\"score\"].sum()/ground_filtered[\"score\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2 of Faiss_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Tkt_Recommendation_sep27_Datasets/ISSUE_DATA.xlsx\")\n",
    "# df.to_csv(\"complete_BR_ISSUE_DATA.csv\",index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='complete_BR_ISSUE_DATA.csv', encoding=\"utf8\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(data,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"faiss_index_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Faiss Indx V4 (Summary only Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"complete_BR_ISSUE_DATA.csv\")\n",
    "# df[\"Summary\"].to_csv(\"BR_ISSUE_DATA_only_Summary.csv\",index=False)\n",
    "loader = CSVLoader(file_path='LLM Similar Ticket Detection/ISSUE_DATA_only_Summary.csv', encoding=\"utf8\")\n",
    "data = loader.load()\n",
    "db = FAISS.from_documents(data,embeddings)\n",
    "db.save_local(\"faiss_index_v4_only_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recommend_score(issue,query):\n",
    "    result_docs = db.similarity_search_with_score(query,k=5)\n",
    "    idxes = [r[0].metadata[\"row\"] for r in result_docs]\n",
    "    recommended_keys = tkt_data1.iloc[idxes,:][\"Key\"].values\n",
    "    ground_truth_keys = dup_df[dup_df[\"Issue\"]==issue].values.reshape(-1)\n",
    "    return ((sum([1 if each in recommended_keys else 0 for each in ground_truth_keys]))/(len(ground_truth_keys)))*100\n",
    "recommend_score(ground_filtered[\"Key\"].values[0],ground_filtered[\"Description\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_filtered[\"score\"] = [recommend_score(row[0],row[1]) for _,row in ground_filtered[[\"Key\",\"Description\"]].iterrows()]\n",
    "ground_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       "50.000000     253\n",
       "100.000000    165\n",
       "75.000000      18\n",
       "0.000000        5\n",
       "83.333333       4\n",
       "62.500000       1\n",
       "58.333333       1\n",
       "66.666667       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_filtered[\"score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.24293154761905"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_filtered[\"score\"].sum()/ground_filtered[\"score\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_docs = db.similarity_search_with_score(df.iloc[0,:][\"Description\"],k=10)\n",
    "for idx,doc in enumerate(result_docs):\n",
    "    print(f\"Recommendation {idx}, score = {doc[1]}%\\n=============================\\n\")\n",
    "    print(doc[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3 of FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='ISSUE_DATA.csv', encoding=\"utf8\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(data,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"faiss_index_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_filtered[\"score\"] = [recommend_score(row[0],row[1]) for _,row in ground_filtered[[\"Key\",\"Description\"]].iterrows()]\n",
    "ground_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.52152423469387"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_filtered[\"score\"].sum()/ground_filtered[\"score\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='paraphrase-xlm-r-multilingual-v1')\n",
    "db_v4 = FAISS.from_documents(data,embeddings)\n",
    "\n",
    "def recommend_score(issue,query,db):\n",
    "    result_docs = db.similarity_search_with_score(query,k=5)\n",
    "    idxes = [r[0].metadata[\"row\"] for r in result_docs]\n",
    "    recommended_keys = tkt_data1.iloc[idxes,:][\"Key\"].values\n",
    "    ground_truth_keys = dup_df[dup_df[\"Issue\"]==issue].values.reshape(-1)\n",
    "    return ((sum([1 if each in recommended_keys else 0 for each in ground_truth_keys]))/(len(ground_truth_keys)))*100\n",
    "\n",
    "ground_filtered[\"score\"] = [recommend_score(row[0],row[1],db_v4) for _,row in ground_filtered[[\"Key\",\"Description\"]].iterrows()]\n",
    "ground_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5376, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "links = pd.read_csv(\"LLM Similar Ticket Detection/Data/links.csv\")\n",
    "cr = \"CR-1159059\"\n",
    "links[links[\"BiraKey\"]==cr][\"LinkBiraKey\"].values\n",
    "links.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
